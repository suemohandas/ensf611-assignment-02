{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e60cc6-65e1-43ee-8276-2f93ab54426b",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: SOUMINI MOHANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f86925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33583c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb337239-0252-495c-b056-f6968b384aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the spam dataset into a feature matrix X and a target vector y\n",
    "X, y = load_spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1991b945-24e9-484a-9823-9b92ede64a93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of feature matrix X:  (4600, 57)\n",
      "The type of feature matrix X:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the feature matrix X \n",
    "# Shape indicates (n_samples, n_features)\n",
    "print(\"The shape of feature matrix X: \", X.shape) \n",
    "print(\"The type of feature matrix X: \", type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848568b2-8e30-42cf-8e5a-8190a1c9c43b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of target vector y:  (4600,)\n",
      "The type of taget vector y:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the target vector y \n",
    "# Shape indicates length (n_samples)\n",
    "print(\"The shape of target vector y: \", y.shape) \n",
    "print(\"The type of taget vector y: \", type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927c9ed5-b647-428e-b5f5-745c0f45db1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.21               0.28           0.50           0.0   \n",
       "1            0.06               0.00           0.71           0.0   \n",
       "2            0.00               0.00           0.00           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.14            0.28              0.21                0.07   \n",
       "1           1.23            0.19              0.19                0.12   \n",
       "2           0.63            0.00              0.31                0.63   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           1.85            0.00              0.00                1.85   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
       "0             0.00            0.94  ...                   0.0         0.00   \n",
       "1             0.64            0.25  ...                   0.0         0.01   \n",
       "2             0.31            0.63  ...                   0.0         0.00   \n",
       "3             0.31            0.63  ...                   0.0         0.00   \n",
       "4             0.00            0.00  ...                   0.0         0.00   \n",
       "\n",
       "   char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0        0.132          0.0        0.372        0.180        0.048   \n",
       "1        0.143          0.0        0.276        0.184        0.010   \n",
       "2        0.137          0.0        0.137        0.000        0.000   \n",
       "3        0.135          0.0        0.135        0.000        0.000   \n",
       "4        0.223          0.0        0.000        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       5.114                         101   \n",
       "1                       9.821                         485   \n",
       "2                       3.537                          40   \n",
       "3                       3.537                          40   \n",
       "4                       3.000                          15   \n",
       "\n",
       "   capital_run_length_total  \n",
       "0                      1028  \n",
       "1                      2259  \n",
       "2                       191  \n",
       "3                       191  \n",
       "4                        54  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the various columns present in the feature matrix X\n",
    "# (i.e., 57 features = 57 columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb72d929-7eb2-47cd-a1bc-ee7297b4923a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    1\n",
       "8    1\n",
       "9    1\n",
       "Name: is_spam, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the 1D target array having only 1 column (i.e., is_spam)\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23509932-bbfe-4cc9-941b-ddbddc97f5a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_freq_make                0\n",
       "word_freq_address             0\n",
       "word_freq_all                 0\n",
       "word_freq_3d                  0\n",
       "word_freq_our                 0\n",
       "word_freq_over                0\n",
       "word_freq_remove              0\n",
       "word_freq_internet            0\n",
       "word_freq_order               0\n",
       "word_freq_mail                0\n",
       "word_freq_receive             0\n",
       "word_freq_will                0\n",
       "word_freq_people              0\n",
       "word_freq_report              0\n",
       "word_freq_addresses           0\n",
       "word_freq_free                0\n",
       "word_freq_business            0\n",
       "word_freq_email               0\n",
       "word_freq_you                 0\n",
       "word_freq_credit              0\n",
       "word_freq_your                0\n",
       "word_freq_font                0\n",
       "word_freq_000                 0\n",
       "word_freq_money               0\n",
       "word_freq_hp                  0\n",
       "word_freq_hpl                 0\n",
       "word_freq_george              0\n",
       "word_freq_650                 0\n",
       "word_freq_lab                 0\n",
       "word_freq_labs                0\n",
       "word_freq_telnet              0\n",
       "word_freq_857                 0\n",
       "word_freq_data                0\n",
       "word_freq_415                 0\n",
       "word_freq_85                  0\n",
       "word_freq_technology          0\n",
       "word_freq_1999                0\n",
       "word_freq_parts               0\n",
       "word_freq_pm                  0\n",
       "word_freq_direct              0\n",
       "word_freq_cs                  0\n",
       "word_freq_meeting             0\n",
       "word_freq_original            0\n",
       "word_freq_project             0\n",
       "word_freq_re                  0\n",
       "word_freq_edu                 0\n",
       "word_freq_table               0\n",
       "word_freq_conference          0\n",
       "char_freq_;                   0\n",
       "char_freq_(                   0\n",
       "char_freq_[                   0\n",
       "char_freq_!                   0\n",
       "char_freq_$                   0\n",
       "char_freq_#                   0\n",
       "capital_run_length_average    0\n",
       "capital_run_length_longest    0\n",
       "capital_run_length_total      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the X dataframe contains any missing or NaN values\n",
    "# sum() indicates the total count of NaN values present in each column \n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "549ae206-7c25-4895-a0be-b18bf83d6427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the target vector y has any missing values \n",
    "# present in its 1 column (i.e., is_spam)\n",
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e7204f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As seen in the above 2 commands, there are no missing or NaN values\n",
    "# in either the feature matrix X or the target vector y\n",
    "# Hence it is not necessary to use a method to fill in missing values\n",
    "# If missing values had existed, we would have used the following commands\n",
    "# assuming that we are filling it with Zeros instead of just dropping \n",
    "# the row or column containing missing or NaN values \n",
    "# X.fillna(0) # For feature matrix X\n",
    "# y.fillna(0) # For target vector y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9bc4a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting is done using the train_test_split utility in SciKit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Create a new feature matrix X_small and a new target vector y_small \n",
    "# using only 5% of the data to train\n",
    "X_small, X_small_t, y_small, y_small_t = train_test_split(X, y, random_state=0, train_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f51b14-7fb9-4610-8e64-a772b8ee9442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of feature matrix X_small:  (230, 57)\n",
      "The shape of target vector y_small:  (230,)\n"
     ]
    }
   ],
   "source": [
    "# Number of samples in X and y were 4600. 5% of 4600 = 230\n",
    "# Hence X_small & y_small contain 230 samples of data \n",
    "print(\"The shape of feature matrix X_small: \", X_small.shape)\n",
    "print(\"The shape of target vector y_small: \", y_small.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb46cdcf-ab53-4769-af0a-8c82f3db5a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose a model class (in this case - Logistic Regression) \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0e98ece-e413-497b-a40d-65bb500ec05d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model with max_iter = 2000 for the first dataset\n",
    "model = LogisticRegression(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "856056d1-725d-42e9-8f5b-6560cf56064d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With X and y, The training accuracy is:  0.9222826086956522\n",
      "With X and y, The validation accuracy is:  0.9369565217391305\n"
     ]
    }
   ],
   "source": [
    "# Implement the above model with X and y dataset\n",
    "# 20& data to test and 80% data to train the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.20)\n",
    "# Fit the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "# Evaluate the model with the testing set and compare it with the training set \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "y_train_pred = model.fit(X_test, y_test).predict(X_train)\n",
    "print(\"With X and y, The training accuracy is: \", accuracy_score(y_train, y_train_pred))\n",
    "print(\"With X and y, The validation accuracy is: \", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ba6d2c-dfba-4fe7-8732-53197d0551ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model with max_iter = 2000 for the second dataset\n",
    "model = LogisticRegression(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e82b42-0d62-488b-ba5c-4539981d01d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address\n",
       "0            0.21               0.28\n",
       "1            0.06               0.00\n",
       "2            0.00               0.00\n",
       "3            0.00               0.00\n",
       "4            0.00               0.00\n",
       "5            0.00               0.00\n",
       "6            0.00               0.00\n",
       "7            0.15               0.00\n",
       "8            0.06               0.12\n",
       "9            0.00               0.00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using only first two columns of X and y\n",
    "X, y = load_spam()\n",
    "X_columns = X[[\"word_freq_make\", \"word_freq_address\"]]\n",
    "X_columns.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7b73c6-6b05-48bd-97e1-e1f703f11e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    1\n",
       "8    1\n",
       "9    1\n",
       "Name: is_spam, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_columns = y\n",
    "y_columns.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2463d1f-3660-4114-af0d-a54a82a6494b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With X_columns and y_columns, The training accuracy is:  0.6377717391304348\n",
      "With X_columns and y_columns, The validation accuracy is:  0.5934782608695652\n"
     ]
    }
   ],
   "source": [
    "# Implement the above model with X_columns and y_columns dataset\n",
    "# 20& data to test and 80% data to train the model\n",
    "X_col_train, X_col_test, y_col_train, y_col_test = train_test_split(X_columns, y_columns, random_state=0, test_size=0.20)\n",
    "# Fit the model on the training set\n",
    "model.fit(X_col_train, y_col_train)\n",
    "# Evaluate the model with the testing set and compare it with the training set \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_col_test_pred = model.fit(X_col_train, y_col_train).predict(X_col_test)\n",
    "y_col_train_pred = model.fit(X_col_test, y_col_test).predict(X_col_train)\n",
    "print(\"With X_columns and y_columns, The training accuracy is: \", accuracy_score(y_col_train, y_col_train_pred))\n",
    "print(\"With X_columns and y_columns, The validation accuracy is: \", accuracy_score(y_col_test, y_col_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "947ac085-9af7-47f8-87b7-2a59435aef76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model with max_iter = 2000 for the third dataset\n",
    "model = LogisticRegression(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa25ad9e-a4d9-4161-bafb-59320936400f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With X_small and y_small, The training accuracy is:  0.8369565217391305\n",
      "With X_small and y_small, The validation accuracy is:  0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "# Implement the above model with X_small and y_small dataset\n",
    "# 20& data to test and 80% data to train the model\n",
    "X_small_train, X_small_test, y_small_train, y_small_test = train_test_split(X_small, y_small, random_state=0, test_size=0.20)\n",
    "# Fit the model on the training set\n",
    "model.fit(X_small_train, y_small_train)\n",
    "# Evaluate the model with the testing set and compare it with the training set \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_small_test_pred = model.fit(X_small_train, y_small_train).predict(X_small_test)\n",
    "y_small_train_pred = model.fit(X_small_test, y_small_test).predict(X_small_train)\n",
    "print(\"With X_small and y_small, The training accuracy is: \", accuracy_score(y_small_train, y_small_train_pred))\n",
    "print(\"With X_small and y_small, The validation accuracy is: \", accuracy_score(y_small_test, y_small_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be4b5c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a8d429c-191e-43e2-b7e8-006720469d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Data size  Training accuracy  Validation accuracy\n",
      "0  (4600, 57)           0.922283             0.936957\n",
      "1   (4600, 2)           0.637772             0.593478\n",
      "2   (230, 57)           0.836957             0.913043\n"
     ]
    }
   ],
   "source": [
    "# results is initially stored in a list\n",
    "results_list = []\n",
    "# Three different datasets for different data sizes \n",
    "datasets = [{\"size\": X.shape, \"X\": X, \"y\": y},\n",
    "    {\"size\": X_columns.shape, \"X\": X_columns, \"y\": y_columns},\n",
    "    {\"size\": X_small.shape, \"X\": X_small, \"y\": y_small}]\n",
    "# A for-loop to store data in the \"results\" list\n",
    "for dataset in datasets:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(dataset[\"X\"], dataset[\"y\"], random_state=0, test_size=0.20)\n",
    "    # Calculating training accuracy\n",
    "    training_accuracy = accuracy_score(y_train, model.fit(X_val, y_val).predict(X_train))\n",
    "    # Calculating validation accuracy\n",
    "    validation_accuracy = accuracy_score(y_val, model.fit(X_train, y_train).predict(X_val))\n",
    "    # Adding results to the DataFrame\n",
    "    results_list.append([dataset[\"size\"], training_accuracy, validation_accuracy])\n",
    "# Creating a DataFrame from the results list\n",
    "results = pd.DataFrame(results_list, columns=[\"Data size\", \"Training accuracy\", \"Validation accuracy\"])\n",
    "# Printing the results DataFrame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07ad0ad3-7578-49ef-a6d5-cfbc259a98a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For Visual representation of the above \"results\", we plot a bar graph \n",
    "import matplotlib.pyplot as plt\n",
    "# Sample data from the results DataFrame\n",
    "data_sizes = results['Data size']\n",
    "training_accuracies = results['Training accuracy']\n",
    "validation_accuracies = results['Validation accuracy']\n",
    "# Create an index for each tick position\n",
    "ind = np.arange(len(data_sizes))\n",
    "# Plotting the bar chart\n",
    "width = 0.4  # Width of the bars\n",
    "# Plotting training accuracy\n",
    "plt.bar(ind - width/2, training_accuracies, width, label='Training Accuracy', color='blue')\n",
    "# Plotting test accuracy\n",
    "plt.bar(ind + width/2, validation_accuracies, width, label='Validation Accuracy', color='red')\n",
    "# Describing the data\n",
    "plt.xlabel('Data Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "# Adding labels to the x-axis\n",
    "plt.xticks(ind, data_sizes)\n",
    "# Adding a legend\n",
    "plt.legend()\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5d215-8712-41e9-98e4-59fcc169e206",
   "metadata": {},
   "source": [
    "Answer 01:\n",
    "Model 1 which used the original feature matrix X and the target vector y had 4600 samples with 57 features. This model had a training accuracy of 92.22% and a validation accuracy of 93.69%. This high training accuracy demonstrates that the model makes accurate predictions indicating low bias. And the very low difference (1.47%) between the training and validation accuracy indicates a low variance and that the model predicts well with new data. It is a well-fitted model that performs well on both the training and validation datasets. \n",
    "\n",
    "Model 2 which used only the first two columns of the original feature matrix i.e., X_columns and the same target vector y had 4600 samples, but only 2 features. This model had a training accuracy of 63.77& and a validation accuracy of 59.34%. The very low training accuracy demonstrates that the model does not fit the training data well indicating a high bias which is likely due to the reduced feature set making the model too simple. The difference (4.43%) between the training and validation accuracy indicates a moderate variance. And the model performs poorly on both the training and validation datasets and is underfitting since only 2 features were considered indicating that the model is too simple to capture the dataâ€™s complexity effectively.\n",
    "\n",
    "Model 3 which used just 5% of the original dataset had 230 samples with 57 features. This model had a training accuracy of 83.69% and a validation accuracy of 91.30%. The model has decent training accuracy demonstrating that certain patterns are accurately captured, if not all. This indicates a moderate bias. The difference (7.61%) between the training and validation accuracy indicates a high variance which may be due to the small size of the dataset. The model potentially overfits even though one would expect the training accuracy to be significantly higher than the validation accuracy in overfitting scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6eee65-4f97-40c5-8e8b-49b05f7c93d1",
   "metadata": {},
   "source": [
    "Answer 02:\n",
    "A false positive happens when the model incorrectly classifies a non-spam message as spam. This means that a legitimate email is flagged as spam and potentially gets moved to the spam folder.\n",
    "\n",
    "A false negative happens when the model incorrectly classifies a spam message as non-spam. This means that a spam email is not flagged as spam and ends up in the user's inbox.\n",
    "\n",
    "The worse condition between the two situations purely is based on the specific use case and the priorities of users. But if I had to pick one, I would say a false positive is worse wherein legitimate emails are classified as spam, thereby causing users to potentially miss important or non-spam messages if they do not regularly check their spam folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d322a6-77d7-42bf-bac2-103af4542a68",
   "metadata": {},
   "source": [
    "Answer: \n",
    "I followed the steps as instructed in this notebook. \n",
    "\n",
    "The following resources were referred to for completing this question - lecture notes, Jupyter notebooks on machine learning, model selection, and linear regression, Python Data Science handbook, and a few of the Jupyter Notebooks related to Pandas from ENSF592. \n",
    "\n",
    "A few challenges that I faced were forgetting the commands for accessing the column or element in a 'DataFrame' vs a 'Series' vs a numpy array. I was able to overcome this hurdle by working on ipython and visualizing a very small dataframe sample and seeing how it varied from a series. And in this assignment Jupyter notebook, having multiple print() statements in between helped me visualize the table or the section of data I was working with, especially how the X and y datasets varied when only parts or subsets of the original X and y were considered. \n",
    "\n",
    "I also had a little trouble plotting the training vs validation accuracy for the different data sizes, since I had represented the data size as a tuple of the number of samples and the number of features. I was unsure as to how I wanted to indicate the tuple in my bar graph - if I just wanted to show the number of samples in each dataset or would it be wiser to show the shape of each dataset! The generative AI helped with the command for bar width and creating an index for the data sizes. I did not have to modify my code for \"results\" per say, since this bar graph was only a part of visual representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ff2e34f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import concrete dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4433662a-96b8-4a19-8c3d-ec103e89470c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the concrete dataset into a feature matrix X and a target vector y\n",
    "X, y = load_concrete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a02ee94c-f77e-4311-af61-8f78688364c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of feature matrix X:  (1030, 8)\n",
      "The type of feature matrix X:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the feature matrix X \n",
    "# Shape indicates (n_samples, n_features)\n",
    "print(\"The shape of feature matrix X: \", X.shape) \n",
    "print(\"The type of feature matrix X: \", type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb4037cf-a598-4e6a-856c-529f3fe64528",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of target vector y:  (1030,)\n",
      "The type of taget vector y:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the target vector y \n",
    "# Shape indicates length (n_samples)\n",
    "print(\"The shape of target vector y: \", y.shape) \n",
    "print(\"The type of taget vector y: \", type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b751b261-5068-4039-8ead-00a2f7688b66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>splast</th>\n",
       "      <th>coarse</th>\n",
       "      <th>fine</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  ash  water  splast  coarse   fine  age\n",
       "0   540.0    0.0  0.0  162.0     2.5  1040.0  676.0   28\n",
       "1   540.0    0.0  0.0  162.0     2.5  1055.0  676.0   28\n",
       "2   332.5  142.5  0.0  228.0     0.0   932.0  594.0  270\n",
       "3   332.5  142.5  0.0  228.0     0.0   932.0  594.0  365\n",
       "4   198.6  132.4  0.0  192.0     0.0   978.4  825.5  360"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the various columns present in the feature matrix \n",
    "# (i.e., 8 features = 8 columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d4354e1-98d5-4ac1-8ed0-9b979581de70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    79.986111\n",
       "1    61.887366\n",
       "2    40.269535\n",
       "3    41.052780\n",
       "4    44.296075\n",
       "5    47.029847\n",
       "6    43.698299\n",
       "7    36.447770\n",
       "8    45.854291\n",
       "9    39.289790\n",
       "Name: strength, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the 1D target array having only 1 column (i.e., strength)\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "693c5fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cement    0\n",
       "slag      0\n",
       "ash       0\n",
       "water     0\n",
       "splast    0\n",
       "coarse    0\n",
       "fine      0\n",
       "age       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the X dataframe contains any missing or NaN values\n",
    "# sum() indicates the total count of NaN values present in each column \n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22cdd029-c27a-4a65-b342-8455b5cfdee1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the target vector y has any missing values \n",
    "# present in its 1 column (i.e., strength)\n",
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20d5f014-06c3-49f7-8523-8d6abbea974f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As seen in the above 2 commands, there are no missing or NaN values\n",
    "# in either the feature matrix X or the target vector y\n",
    "# Hence it is not necessary to use a method to fill in missing values\n",
    "# If missing values had existed, we would have used the following commands\n",
    "# assuming that we are filling it with Zeros instead of just dropping \n",
    "# the row or column containing missing or NaN values \n",
    "# X.fillna(0) # For feature matrix X\n",
    "# y.fillna(0) # For target vector y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5041945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose a model class (in this case - Linear Regression) \n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "828ed2b0-a005-43d8-a5fd-dc41e275215f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model \n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45d3fe73-a87f-4e46-8ae1-cbdffdb71a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implement the above model with X and y dataset\n",
    "# 20& data to test and 80% data to train the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.20)\n",
    "# Fit the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "# Evaluate the model with the testing set and compare it with the training set \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "y_test_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "y_train_pred = model.fit(X_test, y_test).predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "970c038b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With mean squared error(MSE), the training accuracy is:  116.03257386628557\n",
      "With mean squared error(MSE), the validation accuracy is:  95.63533482690427\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean squared error for training and validation data\n",
    "training_mse = mean_squared_error(y_train, y_train_pred)\n",
    "testing_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"With mean squared error(MSE), the training accuracy is: \", training_mse)\n",
    "print(\"With mean squared error(MSE), the validation accuracy is: \", testing_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21891de6-c08d-4283-a89f-a20202c7b1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With R2 score, the training accuracy is:  0.5889230398422326\n",
      "With R2 score, the validation accuracy is:  0.6368981103411243\n"
     ]
    }
   ],
   "source": [
    "# Calculate R2 score for training and validation data\n",
    "training_r2 = r2_score(y_train, y_train_pred)\n",
    "testing_r2 = r2_score(y_test, y_test_pred)\n",
    "print(\"With R2 score, the training accuracy is: \", training_r2)\n",
    "print(\"With R2 score, the validation accuracy is: \", testing_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88d223f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Training Accuracy  Validation Accuracy\n",
      "MSE              116.032574            95.635335\n",
      "R2 Score           0.588923             0.636898\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame({'Training Accuracy': [training_mse, training_r2],\n",
    "    'Validation Accuracy': [testing_mse, testing_r2]}, index=['MSE', 'R2 Score'])\n",
    "# Print the results DataFrame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c5658f-74da-468b-8064-ccb7b48eb754",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Using these 2 linear models have not produced good results. \n",
    "\n",
    "In the case of linear regression using mean squared error (MSE), the MSE values for the training and validation set are relatively very high. And the difference between the training and validation accuracy indicates high variance. This demonstrates that the model's predictions are not close enough to the actual values. This can be due to various factors like inadequate model complexity, poor feature selection, overfitting, or underfitting. The high MSE values may also be due to squaring the errors i.e., differences between predicted and actual values. \n",
    "\n",
    "While in the case of linear regression using R2 score, an R2 score of 0.63 indicates a moderate fit to the data. The difference between the training and validation accuracy indicates moderate variance. An R2 score closer to 1 would have indicated that the model perfectly predicts the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366a046-b64e-4415-ad0b-ec7e1f62ba02",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e19196-8b86-4813-91f3-dd47a87e02e0",
   "metadata": {},
   "source": [
    "Answer: \n",
    "I followed the steps as instructed in this notebook. \n",
    "\n",
    "The following resources were referred to for completing this question - lecture notes, Jupyter notebooks on machine learning, model selection, linear regression, and Regression metrics, Python Data Science handbook, and a few of the Jupyter Notebooks related to Pandas from ENSF592. \n",
    "\n",
    "I was unsure if I had to implement 3 different datasets (X and y; X_col and y_col; and X_small and y_small) as indicated in the spam question above, since Part 2 of this question indicated that I needed to follow steps 1-4 as before. But as I moved down the steps, I realized that I need to be working with just X and y and no other additional datasets. \n",
    "\n",
    "I wanted to know if the MSE values could exceed 100 and I took the help from generative AI to know more about it. I did not have to modify my code for \"results\" per say, since the query was put forth only after running my code and getting an mse value exceeding 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65b584-d217-494f-b7ec-99783b50e9f3",
   "metadata": {},
   "source": [
    "Answer: \n",
    "The high MSE value (116.0) demonstrates that the model has high error rates, and the predicted values are far from the actual values. It might be a sign of underfitting, where the model is too simple to capture the underlying patterns in the data. A lower validation error (95.6) may sometimes be due to the particular split of the data, where the validation set is somehow easier for the model to predict. \n",
    "\n",
    "An R2 score of 0.63 indicates a moderate fit to the data. While the model may be simple (potentially underfitting the training data), it is reasonably effective at making predictions on new, unseen data. Similar to the MSE, the R2 score is higher (0.63) on the validation set, which demonstrates that the model is explaining a higher proportion of variance in the validation data than in the training data. Itâ€™s an unusual pattern because models typically perform better on the training data. \n",
    "\n",
    "Cross-validation or repeated experiments with different splits can be helpful in finding out if this pattern is consistent or just a result of the specific split of the data.\n",
    "\n",
    "Some possible explanations for this unusual behavior may include:\n",
    "\n",
    "The specific random split between training and validation data might have led to a situation where the validation set is somehow â€œeasierâ€ to predict, or the training set contains more complex or noisy examples.\n",
    "\n",
    "The size and features of the training and validation sets can influence these metrics. \n",
    "\n",
    "If the training data contains more noise, outliers, or difficult to predict examples, the model might have a higher error on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c35d6-e7c8-4441-81de-7e9a645c9ceb",
   "metadata": {},
   "source": [
    "Answer: \n",
    "I liked the fact that this exercise/assignment was hands-on and it helped me have a better understanding of what was taught in the lectures/labs and the examples that were discussed in Jupyter Notebooks. \n",
    "\n",
    "I found it interesting to notice that not all datasets deliver the expected \"perfect\" pattern or result. There may be datasets whose patterns deviate from normal. \n",
    "\n",
    "I find it challenging when I have to interpret such unusual results or patterns. This motivates me to learn more about such topics so that I don't feel stuck or clueless as to what the results mean. And working on multiple examples has been really helpful. Also, I would like to put in more effort on the visual aspect (such as plotting graphs) of the data so that graphs could aid me in finding patterns of some kind in the dataset. \n",
    "\n",
    "The only part I found confusing was when we had to predict the y after fitting the model with the training set i.e., \"model.fit(X_train, y_train)\". When we move ahead to evaluate the training and validation accuracy, we use the above model to predict using \"model.predict(X_test)\" for instance. And we use a similar statement for the determination of training accuracy as well. But \"model\" in that statement would still contain the training dataset, thereby making the prediction as say (around 100% accuracy - since it is predicting training using training data itself). To avoid that confusion, I have been explicitly writing \"model.fit(X_train, y_train).predict(X_test)\" for the testing accuracy and a similar statement using the testing data for the training accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e307ba79-e59e-42fd-85c8-9a8fe0145a46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.588923\n",
      "Validation score: 0.636898\n"
     ]
    }
   ],
   "source": [
    "X, y = load_concrete()\n",
    "# Choose a model class (in this case - Ridge regression) \n",
    "from sklearn.linear_model import Ridge\n",
    "# Instantiate the model \n",
    "model = Ridge(alpha=0.001)\n",
    "# Implement the above model with X and y dataset\n",
    "# 20& data to test and 80% data to train the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.20)\n",
    "# Evaluate the model with the testing set and compare it with the training set \n",
    "print(\"Training score: {:.6f}\".format(Ridge(alpha=0.001).fit(X_test, y_test).score(X_train, y_train)))\n",
    "print(\"Validation score: {:.6f}\".format(Ridge(alpha=0.001).fit(X_train, y_train).score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47623d44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.588906\n",
      "Validation score: 0.636937\n"
     ]
    }
   ],
   "source": [
    "X, y = load_concrete()\n",
    "# Choose a model class (in this case - Ridge regression) \n",
    "from sklearn.linear_model import Ridge\n",
    "# Instantiate the model \n",
    "model = Ridge(alpha=100)\n",
    "# Implement the above model with X and y dataset\n",
    "# 20& data to test and 80% data to train the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.20)\n",
    "# Evaluate the model with the testing set and compare it with the training set \n",
    "print(\"Training score: {:.6f}\".format(Ridge(alpha=100).fit(X_test, y_test).score(X_train, y_train)))\n",
    "print(\"Validation score: {:.6f}\".format(Ridge(alpha=100).fit(X_train, y_train).score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1630357-a869-4ccb-b37e-c3d03481668c",
   "metadata": {},
   "source": [
    "As we see in above Ridge regression model, the validation score (0.63) remained the same when compared with the R2 score model (which had a validation score of 0.63 as well). This indicates that the Ridge model performs similarly to the R2 score model. \n",
    "\n",
    "However, changing the alpha value from 0.001 to 100 in the Ridge model had almost negligible effect on the validation score, since a difference of only 0.000039 was noticed. It is worth noting that making such a change in the alpha value is not significant enough since the validation score of the model is not affected whatsoever. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad3281e-5a7a-4487-8548-ee06fc00a173",
   "metadata": {},
   "source": [
    "Now let us consider the case of Lasso Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0f7ed75-8896-4140-bb64-bb243f1845f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.588922\n",
      "Validation set score: 0.636899\n",
      "Number of features used: 8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "print(\"Training set score: {:.6f}\".format(Lasso(alpha=0.001, max_iter=100000).fit(X_test, y_test).score(X_train, y_train)))\n",
    "print(\"Validation set score: {:.6f}\".format(Lasso(alpha=0.001, max_iter=100000).fit(X_train, y_train).score(X_test, y_test)))\n",
    "print(\"Number of features used:\", np.sum(Lasso(alpha=0.001, max_iter=100000).fit(X_train, y_train).coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0f1b332-606c-45aa-a904-66c5040f69db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.406844\n",
      "Validation set score: 0.521070\n",
      "Number of features used: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "print(\"Training set score: {:.6f}\".format(Lasso(alpha=100, max_iter=100000).fit(X_test, y_test).score(X_train, y_train)))\n",
    "print(\"Validation set score: {:.6f}\".format(Lasso(alpha=100, max_iter=100000).fit(X_train, y_train).score(X_test, y_test)))\n",
    "print(\"Number of features used:\", np.sum(Lasso(alpha=100, max_iter=100000).fit(X_train, y_train).coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7f127-fbc6-4cff-90f5-ca8d7f45de1b",
   "metadata": {},
   "source": [
    "As we see in above Lasso Regression model, the validation score (0.63) with alpha=0.001 has remained the same when compared with the R2 score model (which had a validation score of 0.63 as well). This indicates that the Lasso model performs similarly when compared to the R2 score model. \n",
    "\n",
    "However, changing the alpha value from 0.001 to 100 in the Lasso model has significantly reduced the validation score from 0.63 down to 0.52 (and also reduced the number of features considered from 8 to 5), a significant 11% drop. Hence, it can be concluded that the Lasso model with alpha value of 0.001 performs better when compared to the same model with an alpha value of 100 to improve the validation score of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ccebe-1332-4543-a1c7-ec672699a832",
   "metadata": {},
   "source": [
    "Now when we compare the above 2 models i.e., Ridge and the Lasso model, we notice that the validation score obtained from both these models is the same (0.63) as that obtained by the R2 score model (0.63). Hence, it is safe to assume that all 3 models generalize the data reasonably well (i.e., the models are a moderate fit for the data) which is indicated by their validation scores. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
